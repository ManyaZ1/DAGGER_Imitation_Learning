{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "np.bool8 = np.bool_  # Fix for Gym expecting deprecated alias\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # --- Environment Setup ---\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "# Set a global seed\n",
    "SEED = 40\n",
    "\n",
    "# 1. Python / NumPy / Random\n",
    "#random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 2. PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 3. Gym (newer API)\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset(seed=SEED)\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "# --- variables ---\n",
    "MAX_DATASET_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CartPole-v1 Expert ---\n",
    "class ScriptedCartPoleExpert:\n",
    "    def predict(self, obs):\n",
    "        angle = obs[2]\n",
    "        angle_velocity = obs[3]\n",
    "\n",
    "        # Predict where the pole is going\n",
    "        future_angle = angle + 0.5 * angle_velocity\n",
    "\n",
    "        # Move in the direction of the lean (or projected lean)\n",
    "        return 0 if future_angle < 0 else 1\n",
    "\n",
    "\n",
    "    def evaluateExp(self,expert, env, episodes=20):\n",
    "        total = 0\n",
    "        for _ in range(episodes):\n",
    "            obs, _ = env.reset() #many episodes\n",
    "            done = False\n",
    "            while not done:  #many steps per episode \n",
    "                '''Episode End \n",
    "                    The episode ends if any one of the following occurs:\n",
    "                    1.Termination: Pole Angle is greater than ±12°\n",
    "                    2.Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "                    3.Truncation: Episode length is greater than 500 (200 for v0)\n",
    "                    https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
    "                    '''\n",
    "                action = expert.predict(obs)\n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total += reward\n",
    "        return total / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BC/DAGGER Learner (Neural Network) ---\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Train with Behavior Cloning ---\n",
    "def train_bc(expert, num_episodes=10):\n",
    "    states, actions = [], []\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()  # Unpack tuple (obs, info)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = expert.predict(obs)  # Pass obs (numpy array)\n",
    "            states.append(obs)\n",
    "            actions.append(action)\n",
    "            next_obs, _, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated  # New done logic\n",
    "            obs = next_obs\n",
    "    return states, actions\n",
    "\n",
    "# --- Train with DAGGER ---\n",
    "def train_dagger(expert, policy, num_iterations=5):\n",
    "    optimizer = optim.Adam(policy.parameters())\n",
    "    states, actions = [], []\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # --- Rollout ---\n",
    "        trajectory = []\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            trajectory.append(obs)\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(policy(torch.FloatTensor(obs))).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            obs = next_obs\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        # --- Expert labeling + aggregation ---\n",
    "        expert_actions = [expert.predict(s) for s in trajectory]\n",
    "        states.extend(trajectory)\n",
    "        actions.extend(expert_actions)\n",
    "        if len(states) > MAX_DATASET_SIZE:\n",
    "            states = states[-MAX_DATASET_SIZE:]\n",
    "            actions = actions[-MAX_DATASET_SIZE:]\n",
    "        # --- Train policy ---\n",
    "        states_np = np.array(states, dtype=np.float32)\n",
    "        actions_np = np.array(actions, dtype=np.int64)\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(states_np),\n",
    "            torch.from_numpy(actions_np)\n",
    "        )\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        #print(len(dataset))\n",
    "        epochs = min(10, 3 + len(states) // 500)\n",
    "        for epoch in range(epochs):\n",
    "            for batch_states, batch_actions in loader:\n",
    "                logits = policy(batch_states)\n",
    "                loss = nn.CrossEntropyLoss()(logits, batch_actions)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    return (policy, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Evaluate Policy (Fixed) ---\n",
    "def evaluate(policy, num_episodes=10):\n",
    "    total_reward = 0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()  # Unpack tuple\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = torch.argmax(policy(torch.FloatTensor(obs))).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            obs = next_obs\n",
    "    return total_reward / num_episodes\n",
    "\n",
    "def evaluate_policy(policy, episodes=10):\n",
    "    rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(policy(torch.FloatTensor(obs))).item()\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total += reward\n",
    "        rewards.append(total)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Run Experiment ---\n",
    "if __name__ == \"__main__\":\n",
    "    expert = ScriptedCartPoleExpert()\n",
    "    score = expert.evaluateExp(expert, env, episodes=20)\n",
    "    print(f\"Expert average reward: {score}\")\n",
    "    # Train BC\n",
    "    (bc_states, bc_actions) = train_bc(expert)\n",
    "    bc_policy = Policy()\n",
    "    bc_optimizer = optim.Adam(bc_policy.parameters())\n",
    "    \n",
    "    bc_states_np = np.array(bc_states, dtype=np.float32)\n",
    "    bc_actions_np = np.array(bc_actions, dtype=np.int64)\n",
    "\n",
    "    bc_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(bc_states_np),\n",
    "        torch.from_numpy(bc_actions_np)\n",
    "    )\n",
    "    bc_loader = torch.utils.data.DataLoader(bc_dataset, batch_size=32, shuffle=True)\n",
    "    for epoch in range(5):\n",
    "        for states, actions in bc_loader:\n",
    "            logits = bc_policy(states)\n",
    "            loss = nn.CrossEntropyLoss()(logits, actions)\n",
    "            bc_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            bc_optimizer.step()\n",
    "    \n",
    "    # Train DAGGER\n",
    "    dagger_policy = Policy()\n",
    "    dagger_policy, dagger_rewards = train_dagger(expert, dagger_policy, num_iterations=20)\n",
    "\n",
    "    # --- Compare ---\n",
    "    print(f\"BC Average Reward: {evaluate_policy(bc_policy, 60)}\")\n",
    "    print(f\"DAGGER Average Reward: {evaluate_policy(dagger_policy, 60)}\")\n",
    "    \n",
    "    # --- Plot reward over iterations ---\n",
    "    plt.plot(dagger_rewards, marker='o')\n",
    "    plt.xlabel(\"DAGGER Iteration\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"DAGGER Training Progress\")\n",
    "    plt.grid(True)\n",
    "    plt.axhline(evaluate_policy(bc_policy, 30), color='red', linestyle='--', label='BC')\n",
    "    plt.plot(dagger_rewards, marker='o', label='DAgger')\n",
    "    plt.hlines(score,xmin=0,xmax=20, color='black',label='expert')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
